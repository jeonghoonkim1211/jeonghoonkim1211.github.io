---
layout: post
title:  "[AML]Convex Optimization and SVM"
date:   2021-03-10T15:25:52-05:00
author: KJH
categories: Machine_learning
---
Ref) 이글은 동국대학교 박성식 교수님의 '[AIX7026]고급 머신러닝' 강의를 바탕으로 정리하였습니다.


한줄 요약 : SVM의 support vector가 Lagrange multiplier를 통해 결정된다. 

<h2> 1. Primal Optimization Problem(object function) </h2>

![image](https://user-images.githubusercontent.com/43257397/110230219-c9864d80-7f52-11eb-9ef2-437f96932843.png)


object function를 최소화하는 문제에서 제약은 두 종류인데 부등호와 등호이다. 



<h2> 2. Lagrangian function </h2>

![image](https://user-images.githubusercontent.com/43257397/110230352-baec6600-7f53-11eb-8391-d951e3e45bcb.png)

Convex Optimization 문제에서 Lagrange multiplier를 이용하여 Lagrangian function를 정의한다.


<h2> 3. Lagrange dual function </h2>


![image](https://user-images.githubusercontent.com/43257397/110230618-90031180-7f55-11eb-9c23-23aa8a7a6679.png)


object function의 x를 최소화하듯 Lagrangian을 이용하여 x를 최소화 하면 람다와 누만의 함수로 표현가능한 Lagrange dual function을 정의할 수 있다. 
 

 
 <h2> 4. Lagrangian dual Problem </h2>
 
 ![image](https://user-images.githubusercontent.com/43257397/110230725-33ecbd00-7f56-11eb-9fcd-262f32308a23.png)


Convex Optimization 문제에서 Lagrangian dual function을 최대화하는 문제는 object function의 x를 최소화하는 문제의 해와 같다.
 
  <h2>  5. KKT conditions </h2>

<h4>  1) primal constraints </h4>


![image](https://user-images.githubusercontent.com/43257397/110231130-1d943080-7f59-11eb-8355-d6c2b05b9246.png)


<h4> 2) dual constraints :  Lagrange multiplier 람다는 nonnegative. </h4>

<h4>  3) complementary slackness  </h4>

 
 ![image](https://user-images.githubusercontent.com/43257397/110231194-6e0b8e00-7f59-11eb-8d36-a5e7068097ed.png)

만약(not if only) strong duality이고 x,lamda,nu가 optimal하면,
x,lamda,nu는 KKT conditions를 만족한다.


 <h2> 6. SVM  </h2>
 
 <h4>  (1) 정의  </h4>
 
 ![image](https://user-images.githubusercontent.com/43257397/110231303-584a9880-7f5a-11eb-8d62-3f423d712b1d.png)


xn까지 중 가장 가까운 점의 margin을 최대화


<h4>  (2) prediction  </h4>

![image](https://user-images.githubusercontent.com/43257397/110231345-95af2600-7f5a-11eb-8ba6-e4836afea888.png)


<h4>  (3) Dual problem for convex optimization  </h4>

tn = +-1 일때,

![image](https://user-images.githubusercontent.com/43257397/110231364-be372000-7f5a-11eb-99c0-612bf1be0d53.png)


<h4>  (4) Lagrangian function  </h4>

![image](https://user-images.githubusercontent.com/43257397/110231402-ede62800-7f5a-11eb-9eff-209c8ae3cfed.png)


<h4>  (5)w, a  </h4>

![image](https://user-images.githubusercontent.com/43257397/110231416-fd657100-7f5a-11eb-96e5-b24d83c32286.png)

델y = 0

<h4> (6)kernel  </h4>

![image](https://user-images.githubusercontent.com/43257397/110231477-4f0dfb80-7f5b-11eb-833a-4005e72e8081.png)


<ul>
  <li>an과 이후단은 complementary slackness관계</li>
  <li>k(x,xn)에서 x는 새로운데이터, trainning sample xn과의 내적관계</li>
  <li>tn*y(xn)-1<0 이면-> prediction에 영향없고, support vector 아님</li>
</ul>


